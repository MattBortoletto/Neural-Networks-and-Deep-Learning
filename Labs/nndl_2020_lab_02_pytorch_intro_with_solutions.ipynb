{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nndl_2020_lab_02_pytorch_intro_with_solutions.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPVdBNOvgwF72U35faxHSwg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gRboflunHqye"},"source":["#NEURAL NETWORKS AND DEEP LEARNING\n","> M.Sc. ICT FOR LIFE AND HEALTH\n","> \n","> Department of Information Engineering\n","\n","> M.Sc. COMPUTER ENGINEERING\n",">\n","> Department of Information Engineering\n","\n","> M.Sc. AUTOMATION ENGINEERING\n",">\n","> Department of Information Engineering\n"," \n","> M.Sc. PHYSICS OF DATA\n",">\n","> Department of Physics and Astronomy\n"," \n","> M.Sc. COGNITIVE NEUROSCIENCE AND CLINICAL NEUROPSYCHOLOGY\n",">\n","> Department of General Psychology\n","\n","---\n","A.A. 2020/21 (6 CFU) - Dr. Alberto Testolin, Dr. Matteo Gadaleta\n","---\n","\n","\n","##Lab. 02 - Introduction to PyTorch"]},{"cell_type":"code","metadata":{"id":"ODcSwFcDXrtD"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qweR50Z9anMW"},"source":["# Tensors"]},{"cell_type":"markdown","metadata":{"id":"SZk77zMOhPk8"},"source":["Tensors are the main object to interact with the PyTorch library. They are similar to NumPyâ€™s arrays, with the addition being that Tensors can also be used on a GPU to accelerate computing."]},{"cell_type":"code","metadata":{"id":"BSUg910NarHz"},"source":["x = torch.tensor([1, 2, 3])\n","print(x)\n","print(x.type())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eNsBpFWjhjOM"},"source":["The data type is inferred automatically, but it is always a good practice to ensure that the data type is what expected."]},{"cell_type":"code","metadata":{"id":"dAfdcmXia7U_"},"source":["x = torch.tensor([1, 2, 3]).float()\n","print(x)\n","print(x.type())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dHGkCBmWh0yM"},"source":["Most methods are similar to the numpy equivalent. For example:"]},{"cell_type":"code","metadata":{"id":"gxPCoqF9a74Y"},"source":["x = torch.rand(4, 5)\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJKdobg2a8Kf"},"source":["x = torch.zeros(4, 5).long()\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAUFFpcga8dH"},"source":["x = torch.ones(4, 5).double()\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FbNS6-jEiRnR"},"source":["a = torch.rand(3, 4)\n","b = torch.rand(3, 4)\n","\n","c = torch.add(a, b)\n","print(c)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRyt4n9car9G"},"source":["# Numpy bridge"]},{"cell_type":"markdown","metadata":{"id":"B0gH9l88h-9I"},"source":["It is very easy to convert a numpy array to a tensor and vice-versa."]},{"cell_type":"code","metadata":{"id":"MqebOyCvasTN"},"source":["# Define a numpy array\n","np_x = np.array([1,2,3,4,5], dtype=np.float32)\n","print(np_x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NYjy4whibKH2"},"source":["# Convert to torch tensor\n","torch_x = torch.from_numpy(np_x)\n","print(torch_x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqvgzBK7bKb2"},"source":["# Go back to numpy\n","np_x2 = torch_x.numpy()\n","print(np_x2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6amfGGTeatvM"},"source":["# Operations on GPU"]},{"cell_type":"markdown","metadata":{"id":"A4wriIShiVYH"},"source":["One of the biggest benefit of PyTorch is that you can perform operations on GPU without changing the code.\n","\n","- The following code first check if a GPU is available.\n","- If available, move the tensors *a* and *b* to the GPU memory.\n","- The sum is performed on GPU, if available, otherwise on CPU, but the result will be exactly the same."]},{"cell_type":"code","metadata":{"id":"VCc1OP2gauBM"},"source":["a = torch.rand(3, 4)\n","b = torch.rand(3, 4)\n","\n","# Check if a cuda GPU is available\n","if torch.cuda.is_available():\n","    print('GPU availble')\n","    # Define the device (here you can select which GPU to use if more than 1)\n","    device = torch.device(\"cuda\")\n","    # Move previous a and b tensors to the GPU\n","    a = a.to(device)\n","    b = b.to(device)\n","else:\n","    print('GPU not availble')\n","\n","# The operation on a and b will be executed on GPU, if available\n","c = a + b\n","# Move the result tensor back to CPU\n","c = c.cpu()\n","print(c)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1F4G1G64aulO"},"source":["# Autograd"]},{"cell_type":"markdown","metadata":{"id":"iFr7HVkcj7yz"},"source":["PyTorch is able to automatically compute gradients on tensors that have been defined with the *requires_grad* flag. There's no need to say how much this is important for deep learning!\n","\n","But this functionality can be actually used for any gradient, even simple equations like $z = 3 x^2 + y^3$\n","\n","Let's try with $x=2$ and $y=3$"]},{"cell_type":"code","metadata":{"id":"uNHCD_EIau3G"},"source":["# Define operations\n","x = torch.tensor([2.0], requires_grad=True).float()\n","y = torch.tensor([3.0], requires_grad=True).float()\n","z = 3 * x**2 + y**3\n","# Backward\n","z.backward()\n","# Print gradients\n","print('dz/dx evaluated in %f: %f' % (x, x.grad)) # dz/dx = 6 * x = 12\n","print('dz/dy evaluated in %f: %f' % (y, y.grad)) # dz/dy = 3 * y^2 = 27"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hi-SuAjUkiWH"},"source":["The result is correct, in fact:\n","\n","$$\n","z = 3 x^2 + y^3\\\\\n","\\frac{dz}{dx} = 6 x = 12\\\\\n","\\frac{dz}{dy} = 3 y^2 = 27\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"XE3qIrFNlPfk"},"source":["This also works for chained operations"]},{"cell_type":"code","metadata":{"id":"rJEhCxdqbiKo"},"source":["# Define operations\n","x = torch.tensor([2.0], requires_grad=True).float()\n","y = 3 * x**2\n","z = 2 * y**2\n","# Backward\n","z.backward()\n","# Print gradients\n","print('dz/dx evaluated in %f: %f' % (x, x.grad)) \n","# dz/dx =\n","# = (dz/dy) * (dy/dx) = \n","# = (4*y) * (6*x)  = \n","# = (4*3*x^2) * (6*x) =\n","# = 72 * x^3\n","print(f\"Correct value: {72 * 2**3}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mdjvTBO_llVn"},"source":["Useful deep learning functions are already implemented. Let's try with a sigmoid and its derivative."]},{"cell_type":"code","metadata":{"id":"0ivtXgiNlb9h"},"source":["x = torch.linspace(-10, 10, 1000, requires_grad=True)\n","y = torch.sigmoid(x)\n","y.sum().backward()\n","\n","plt.plot(x.data.numpy(), y.data.numpy())\n","plt.plot(x.data.numpy(), x.grad.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kuOarw975RQq"},"source":["# Network training procedure"]},{"cell_type":"markdown","metadata":{"id":"LcEtUGA0zePr"},"source":["## Network Definition"]},{"cell_type":"markdown","metadata":{"id":"KvblBA0C1tcu"},"source":["A network is defined by extending the *torch.nn.module* class. The basic structure is:\n","\n","```\n","class Net(nn.Module):\n","    \n","    def __init__(self, input_parameters):\n","        super().__init__() # This executes the parent __init__ method\n","        [...]\n","\n","    def forward(self, x, optional_parameters):\n","        [...]\n","        return out # return the output of the network\n","```\n","\n","You need to define two methods:\n","*   **\\_\\_init\\_\\_**: The constructor method. This is exectuted when the object is initialized (no need to call it explicitly). Here you have to instantiate all the network's parameters. PyTorch provides utility functions to easily initialize most of the commonly used deep learning layers.\n","*   **forward**: Here you define the forward pass of the network, from the input *x* to the output (the method must return the network output). You just need to define the forward part, the back-propagation is automatically tracked by the framework!"]},{"cell_type":"markdown","metadata":{"id":"Dhnl9oxR3daH"},"source":["Let's try with a simple single layer feed-forward network. A fully connected layer is initialized with the utility method *torch.nn.Linear*. **Keep in mind that the activation function is not included!**\n","\n","Here you can find all the available layers: [https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)"]},{"cell_type":"code","metadata":{"id":"6tXqq4f54Cae"},"source":["class Net(nn.Module):\n","    \n","    def __init__(self, Ni, Nh, No):\n","        \"\"\"\n","        Ni - Input size\n","        Nh - Neurons in the hidden layer\n","        No - Output size\n","        \"\"\"\n","        super().__init__()\n","        \n","        print('Network initialized')\n","        self.fc1 = nn.Linear(in_features=Ni, out_features=Nh)\n","        self.out = nn.Linear(in_features=Nh, out_features=No)\n","        self.act = nn.Sigmoid()\n","        \n","    def forward(self, x):\n","        x = self.act(self.fc1(x))\n","        x = self.out(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3-OBirD5d5Y"},"source":["## Network Initialization"]},{"cell_type":"markdown","metadata":{"id":"nGrnSEP84u_4"},"source":["Now we can initialize the network\n","\n","\n","\n","---\n","\n","*HINT*\n","\n","For reproducibility, it is always recommended to set a manual seed. In this way the randomly initialized network's parameters will be always the same. Try to disable it to see how the initialized weights change every time you redefine the network object.\n"]},{"cell_type":"code","metadata":{"id":"CsVbCcFv5kJM"},"source":["# Network parameters\n","Ni = 1\n","Nh = 32\n","No = 1\n","\n","torch.manual_seed(0)\n","net = Net(Ni, Nh, No)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jkkSRVMp5viC"},"source":["As you can see from the *print* output, the *\\_\\_init\\_\\_* method has been executed."]},{"cell_type":"markdown","metadata":{"id":"bHaDo1zw6Dtm"},"source":["You can see the network structure using the *print* function:"]},{"cell_type":"code","metadata":{"id":"tbJehKq66CKO"},"source":["print(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IddCFlO96_ij"},"source":["You can easily access the parameter values:\n","\n","Documetation:\n","\n","* [nn.Module.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=parameters#torch.nn.Module.parameters)\n","* [nn.Module.named_parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=named_param#torch.nn.Module.named_parameters)\n","\n","---\n","*HINTS*\n","\n","Check the documentation to see how pyTorch initialize the parameters. This clearly depends on the layer's type."]},{"cell_type":"code","metadata":{"id":"b2GmUZjE6QIM"},"source":["for param_name, param_values in net.named_parameters():\n","  param_values = param_values.detach().numpy() # Transform to numpy array, if needed\n","  print(\"\\n####################################\")\n","  print(\"####################################\")\n","  print(f\"#### PARAMETER NAME: {param_name}\")\n","  print(\"####################################\")\n","  print(f\"#### PARAMETER SHAPE: {param_values.shape}\")\n","  print(f\"#### PARAMETER VALUES: {param_values}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLrANQkcBgxp"},"source":["In this case we have just 32 biases and 32 weights for the hidden layer and 32 weights and 1 bias for the output layer."]},{"cell_type":"markdown","metadata":{"id":"qfEjiSlkASLU"},"source":["## Process input"]},{"cell_type":"markdown","metadata":{"id":"IFMv0N6nB15b"},"source":["Let's define a batch of 128 random input values.\n","\n","Since we have a single input neuron, the input shape is be (128, 1)."]},{"cell_type":"code","metadata":{"id":"ye2TnCJvCGof"},"source":["x = torch.rand(128, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VTj9XnnvCMrZ"},"source":["Processing the input is very simple, just call the network object with the data. PyTorch will call the forward function internally and automatically track the gradients."]},{"cell_type":"code","metadata":{"id":"RpWL3hcFCkCb"},"source":["out = net(x)\n","print(f\"OUTPUT VALUES: {out}\")\n","print(f\"OUTPUT SHAPE: {out.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4gJJEo-oEFFN"},"source":["### Disabling gradient tracking"]},{"cell_type":"markdown","metadata":{"id":"I7aeQ_DFERWQ"},"source":["PyTorch tracks gradients by default (it does not know if you are training or testing). During testing, it is always recommended to disable the backpropagation to speed-up the computation. This can be easily achieved with this command:"]},{"cell_type":"code","metadata":{"id":"nt5Tjg5REtKu"},"source":["with torch.no_grad():\n","  out = net(x)\n","  \n","print(f\"OUTPUT SHAPE: {out.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZuiM_3NPENM6"},"source":["### Run on GPU"]},{"cell_type":"markdown","metadata":{"id":"8dKfluI5E3-f"},"source":["To run the computation on GPU you need to:\n","\n","*   Transfer the network parameters to the GPU memory\n","*   Transfer the input data to the GPU memory\n","*   If needed, transfer back the output to the CPU memory\n"]},{"cell_type":"code","metadata":{"id":"xe31iR4qFI9y"},"source":["### Check if a cuda GPU is available\n","if torch.cuda.is_available():\n","    print('GPU availble')\n","    # Define the device (here you can select which GPU to use if more than 1)\n","    device = torch.device(\"cuda\")\n","else:\n","    print('GPU not availble')\n","    device = torch.device(\"cpu\")\n","\n","print(f\"SELECTED DEVICE: {device}\")\n","\n","### Transfer the network parameters to the GPU memory (if available)\n","net.to(device)\n","\n","### Transfer the input data to the GPU memory (if available) and compute output\n","out = net(x.to(device))\n","\n","print(f\"OUTPUT SHAPE: {out.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ffHlNvBYHLvS"},"source":["## Compute the loss"]},{"cell_type":"markdown","metadata":{"id":"k3T0kYcqKZ6V"},"source":["PyTorch already implements all the loss functions commonly used in any machine learning and deep learning problems, both for regression and classification.\n","\n","Here the complete list of the available loss functions: [loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n","\n","Check the documentation for all the available parameters, which clearly depend on the specific loss.\n","\n","Let's define an MSELoss (with default parameters) as example:\n"]},{"cell_type":"code","metadata":{"id":"yFYj-rQ5LEqF"},"source":["# Define the loss function\n","loss_function = nn.MSELoss()\n","\n","# Evaluate the loss function\n","a = torch.zeros(100)\n","b = torch.rand(100)\n","loss_value = loss_function(a, b)\n","print(f\"Computed loss: {loss_value}\")\n","\n","expected_mse = np.mean(a.numpy()**2 + b.numpy()**2)\n","print(f\"Expected MSE: {expected_mse}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7oeo4IthL6_y"},"source":["## Backpropagation"]},{"cell_type":"markdown","metadata":{"id":"b-V0bRU9S0_r"},"source":["Since PyTorch automatically tracks the gradients, the backpropagation step can be done is a single line of code by calling the *.backward()* method of the loss tensor.\n","\n","Before that, you need to be sure that there are no gradients accumulated by previous operations by calling the method *.zero_grad()* of the network object.\n","\n","Let's try with some random input and labels."]},{"cell_type":"code","metadata":{"id":"3qsuPQTgTgKT"},"source":["x = torch.rand(128, 1).to(device)\n","label = torch.rand(128, 1).to(device)\n","\n","# Forward pass\n","out = net(x)\n","\n","# Compute loss\n","loss = loss_function(out, label)\n","\n","# Backpropagation\n","net.zero_grad()\n","loss.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-WJuFwa2Ua9E"},"source":["Let's check the just evaluated gradients, for example on the parameters of the first layer."]},{"cell_type":"code","metadata":{"id":"0TLjdbxJUnzT"},"source":["params = next(net.fc1.parameters())\n","print(\"#########################\")\n","print(\"#########################\")\n","print(f\"VALUES\")\n","print(\"#########################\")\n","print(params)\n","\n","print(\"\\n#########################\")\n","print(\"#########################\")\n","print(f\"GRADIENTS\")\n","print(\"#########################\")\n","print(params.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2aT9hD6VydG"},"source":["## Update the weights (Optimizer)"]},{"cell_type":"markdown","metadata":{"id":"E8Ha4afUX91C"},"source":["Now that we have the gradients, we can also manually implement the simplest gradient descent just by multiplying the gradients by a learning rate, and then add this contribution to the original weights.\n","\n","However, PyTorch already implements much more complex approaches which also consider higher order momentum and other improvments for a better optimization.\n","\n","You can find all the available optimizers in the *torch.nn.optim module*. Check the documentation here: [optim](https://pytorch.org/docs/stable/optim.html)\n","\n","\n","For example, let's use the Adaptive Moment (Adam) algorithm.\n","\n","[Adam paper](https://arxiv.org/abs/1412.6980)"]},{"cell_type":"code","metadata":{"id":"SSGDFPgNZFsi"},"source":["# Random training data\n","x = torch.rand(128, 1).to(device)\n","label = torch.rand(128, 1).to(device)\n","\n","# Define the loss function\n","loss_function = nn.MSELoss()\n","\n","# Define the optimizer\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","\n","# Forward pass\n","out = net(x)\n","\n","# Compute loss\n","loss = loss_function(out, label)\n","\n","# Backpropagation\n","net.zero_grad()\n","loss.backward()\n","\n","# Update the weights\n","print(f\"A specific weight before update: {next(net.fc1.parameters())[0]}\")\n","optimizer.step()\n","print(f\"A specific weight after update: {next(net.fc1.parameters())[0]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PM_pV-lxaDIz"},"source":["As you can see from the print outputs, the optimizer successfully updated the weight.\n","\n","The *.step()* method of the optimizer is where the optimizer actually update the weights.\n","\n","When you define the optimizer, you have to provide:\n","\n","* The parameters that the optimizer should update during the update step (in this case all the network parameters, but it can also be a subset for more complex network implementations).\n","* Additional parameters that depend on the specific optimizier. In this case we are just providing a learning rate."]},{"cell_type":"markdown","metadata":{"id":"xTdKUJV_jZ3M"},"source":["# Dataset and Dataloader"]},{"cell_type":"markdown","metadata":{"id":"b7-EPDN2q_6G"},"source":["## The Dataset class"]},{"cell_type":"markdown","metadata":{"id":"Nb-uAPGarKAM"},"source":["`torch.utils.data.Dataset` is an abstract class representing a dataset. Your custom dataset should inherit `Dataset` and override the following methods:\n","\n","* `__init__` to initialize your dataset. For example, if your dataset fits in memory, you can load the entire dataset in a list, or you can just store the list of dataset files.\n","* `__len__` so that len(dataset) returns the size of the dataset.\n","* `__getitem__` to support indexing such that `dataset[i]` can be used to get  the i-th sample\n","\n","Therefore, the structure of the class is:\n","\n","```\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, init_parameters, transform=None):\n","        self.transform = transform\n","        [...]\n","\n","    def __len__(self):\n","        [...]\n","\n","    def __getitem__(self, idx):\n","        [...]\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample\n","```\n","\n","Typically, a `transform` function is provided during initialization. This function is applied to each sample at runtime, so it is executed every time you load a sample from the dataset. This is really helpful, for example, to add random data transformation during training, such as random image rotation, random noise..."]},{"cell_type":"markdown","metadata":{"id":"WCK9Ga7cxJcL"},"source":["Let's create a `Dataset` class that loads data from a csv file with two columns, the first is the input (single value), and the second is the output (label)."]},{"cell_type":"markdown","metadata":{"id":"UsZJ1ytMxrn9"},"source":["First, we create some example files. We generate data using the same quadratic function used in the previous lab.\n","\n","\n","---\n","\n","*HINT*\n","\n","You can use the `pandas` package to deal with csv files and easily map them into pandas `DataFrame` objects.\n","\n","* [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n","* [pandas.DataFrame.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html?highlight=to_csv#pandas.DataFrame.to_csv)"]},{"cell_type":"code","metadata":{"id":"enlqPkqrxrMZ"},"source":["def poly_model(x, beta, noise_std=0):\n","    \"\"\"\n","    INPUT\n","        x: x vector\n","        beta: polynomial parameters\n","        noise_std: enable noisy sampling (gaussian noise, zero mean, noise_std std)\n","    \"\"\"\n","    pol_order = len(beta)\n","    x_matrix = np.array([x**i for i in range(pol_order)]).transpose()\n","    y = np.matmul(x_matrix, beta)\n","    noise = np.random.randn(len(y)) * noise_std\n","    return y + noise\n","\n","beta_true = [-1.45, 1.12, 2.3]\n","noise_std = 0.2\n","np.random.seed(4)\n","\n","### Train data\n","num_train_points = 20\n","x_train = np.random.rand(num_train_points)\n","y_train = poly_model(x_train, beta_true, noise_std)\n","with open('train_data.csv', 'w') as f:\n","  data = [f\"{x},{y}\" for x, y in zip(x_train, y_train)]\n","  f.write('\\n'.join(data))\n","    \n","### Test data\n","num_test_points = 20\n","x_test = np.random.rand(num_test_points)\n","y_test = poly_model(x_test, beta_true, noise_std)\n","with open('test_data.csv', 'w') as f:\n","  data = [f\"{x},{y}\" for x, y in zip(x_test, y_test)]\n","  f.write('\\n'.join(data))\n","\n","  \n","### Plot\n","plt.figure(figsize=(12,8))\n","x_highres = np.linspace(0,1,1000)\n","plt.plot(x_highres, poly_model(x_highres, beta_true), color='b', ls='--', label='True data model')\n","plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n","plt.plot(x_test, y_test, color='g', ls='', marker='.', label='Test data points')\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.grid()\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uEHfHVM2zf68"},"source":["Now we can define our `Dataset` class."]},{"cell_type":"code","metadata":{"id":"W4usa0KIrQL0"},"source":["class CsvDataset(Dataset):\n","\n","  def __init__(self, csv_file, transform=None):\n","    \"\"\"\n","    Args:\n","        csv_file (string): Path to the csv file.\n","        transform (callable, optional): Optional transform to be applied\n","            on a sample.\n","    \"\"\"\n","    self.transform = transform\n","    # Read the file and split the lines in a list\n","    with open(csv_file, 'r') as f:\n","      lines = f.read().split('\\n')\n","    # Get x and y values from each line and append to self.data\n","    self.data = []\n","    for line in lines:\n","      sample = line.split(',')\n","      self.data.append((float(sample[0]), float(sample[1])))\n","    # Now self.data contains all our dataset.\n","    # Each element of the list self.data is a tuple: (input, output)\n","\n","  def __len__(self):\n","    # The length of the dataset is simply the length of the self.data list\n","    return len(self.data)\n","\n","  def __getitem__(self, idx):\n","    # Our sample is the element idx of the list self.data\n","    sample = self.data[idx]\n","    if self.transform:\n","        sample = self.transform(sample)\n","    return sample"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77ME02ST4cYC"},"source":["Now we can instantiate our training and test dataset objects."]},{"cell_type":"code","metadata":{"id":"Nuw86bFW4ifF"},"source":["train_dataset = CsvDataset('train_data.csv')\n","test_dataset = CsvDataset('test_data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TdkdKi6I4n-u"},"source":["We can directly get data from the `Dataset` object in the same way we do with lists."]},{"cell_type":"code","metadata":{"id":"d_pr5S1I4_7O"},"source":["# Direct access (element at index 3, i.e. 4th element, go check the 4th line in the csv file)\n","print(train_dataset[3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j9OC5L6b5PC7"},"source":["# Iterate the entire dataset\n","for sample in train_dataset:\n","  print(sample)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTsZT-w-6Fae"},"source":["## Data transformation"]},{"cell_type":"markdown","metadata":{"id":"Mnc8FauD7ioK"},"source":["Now that we have a dataset to work with, we can move to creating custom transformations. A suite of transformations used at training time is typically referred to as data augmentation and is a common practice for modern model development.\n","\n","One issue common in handling computer vision datasets, for example, is that the samples may not all be the same size. Most neural networks expect the images of a fixed size. Therefore, you will need to write some prepocessing code to rescale and/or crop the image. \n","\n","You can find more details here: [https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html](https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html)\n","\n","In this case we just need to transform our data points into PyTorch **float32** tensors.\n","\n","To do so we write **callable class** instead of simple functions so that parameters of the transform need not be passed everytime itâ€™s called. \n","\n","For this, we just need to implement `__call__` method and if required, `__init__` method. We can then use a transform like this:\n","\n","```\n","tsfm = Transform(params)\n","transformed_sample = tsfm(sample)\n","```"]},{"cell_type":"code","metadata":{"id":"5ViTaeAl-qh_"},"source":["class ToTensor(object):\n","    \"\"\"Convert sample to Tensors.\"\"\"\n","\n","    def __call__(self, sample):\n","        x, y = sample\n","        return (torch.Tensor([x]).float(),\n","                torch.Tensor([y]).float())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I_oZ5DnQACe0"},"source":["Let's check if it works with a custom sample."]},{"cell_type":"code","metadata":{"id":"Tx1dGWkm_OfV"},"source":["to_tensor = ToTensor()\n","sample = (2.3, 3.5)\n","print(to_tensor(sample))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QzWPu20wAxYC"},"source":["You can concatenate multiple transformations (e.g. rescale image -> crop image -> add random noise -> tranform to tensor), using the utility function `torchvision.transforms`. In this case we have just a single tranform, but we use it anyway."]},{"cell_type":"code","metadata":{"id":"XEbIOG9dBbo3"},"source":["composed_transform = transforms.Compose([\n","  ToTensor(),\n","  # Other transformations, if required\n","  ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWsb_EJsBqQh"},"source":["# Test the composed transform\n","sample = (2.3, 3.5)\n","print(composed_transform(sample))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qxbIzH4NAVvK"},"source":["To include the transformation in the previous defined dataset every time we get a sample, we can use the transform parameter of the dataset:"]},{"cell_type":"code","metadata":{"id":"bhME6qGuAmzE"},"source":["train_dataset = CsvDataset('train_data.csv', transform=composed_transform)\n","print(train_dataset[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AnKEarqZB133"},"source":["As you can see, now the dataset returns tensor data."]},{"cell_type":"markdown","metadata":{"id":"OhnQhtz-SGCq"},"source":["### Exercise"]},{"cell_type":"markdown","metadata":{"id":"wkirKQlASVJJ"},"source":["Create a composed transform that:\n","\n","1.   Convert sample to tensors\n","2.   Add a random noise (zero mean and configurable std) to the x value of the sample"]},{"cell_type":"code","metadata":{"id":"kgP_7JRoQ7Xr"},"source":["class AddNoise(object):\n","    \"\"\"Add random noise to sample.\"\"\"\n","\n","    def __init__(self, noise_std):\n","        self.noise_std = noise_std\n","\n","    def __call__(self, sample):\n","        x, y = sample\n","        noise = torch.randn(x.shape) * self.noise_std\n","        return (x + noise, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyMfkuwxR0Yf"},"source":["# Create the composed transform\n","composed_transform = transforms.Compose([\n","  ToTensor(),\n","  AddNoise(noise_std=1.0)\n","  ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0iv9WNkS_aB"},"source":["# Test the composed transform\n","sample = (2.3, 3.5)\n","print(composed_transform(sample))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9wIod3nF5XwW"},"source":["## Dataloader"]},{"cell_type":"markdown","metadata":{"id":"lUkV8Lfk5bl2"},"source":["Creating a `Dataset` class may seem unnecessary for the most basic problems. But it really helps when the dataset and the training procedure start to get more complex.\n","\n","One of the most useful benefit of defining a `Dataset` class is the possiblity to use the PyTorch `Dataloader` module.\n","\n","By operating on the dataset directly, we are losing out on a lot of features by using a simple for loop to iterate over the data. In particular, we are missing out on:\n","\n","* Batching the data\n","* Shuffling the data\n","* Load the data in parallel using multiprocessing workers.\n","\n","`torch.utils.data.DataLoader` is an iterator which provides all these features. Parameters used below should be clear."]},{"cell_type":"code","metadata":{"id":"67Xta9_G6pe1"},"source":["train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n","\n","for i_batch, sample_batched in enumerate(train_dataloader):\n","  print('#################')\n","  print(f'# BATCH {i_batch}')\n","  print('#################')\n","  x_batch = sample_batched[0]\n","  label_batch = sample_batched[1]\n","  print(f\"INPUT DATA (shape: {x_batch.shape})\")\n","  print(x_batch)\n","  print(f\"LABELS (shape: {label_batch.shape})\")\n","  print(label_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pp12qAVgCYOh"},"source":["It automatically created batches, shuffled the data and loaded the samples simultaneously from 4 parallel processes!"]}]}